{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from utils.model_analysis_nets import LeNet, VGG16, ResNet20\n",
    "from utils.load import getGradients, getTotalLength, initDataset, minimizeProduct\n",
    "from utils.load import getSamples, getTopofeature, extractWeights\n",
    "from utils.pcode import isProperSuperset, findDifferentElements, flatten, findLargestGaps, grouping, simpleGrouping\n",
    "from CKA import linear_CKA, kernel_CKA\n",
    "import perscode\n",
    "\n",
    "import numpy as np\n",
    "import sympy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, fclusterdata\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import gif\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import gudhi as gd\n",
    "from ripser import Rips\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "trans_mnist = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trans_cifar10_val = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                             std=[0.229, 0.224, 0.225])])\n",
    "dataset_test = datasets.CIFAR10('data/cifar10/', train=False, download=True, transform=trans_cifar10_val)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset_test, batch_size=32,\n",
    "                num_workers=2, pin_memory=True, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounds [36, 34, 32, 30] 4\n",
      "attack [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] 40\n",
      "normal [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 60\n",
      "640\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# attack_pth = \"./LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern09-24--21-13-11/local_attack_save/\"\n",
    "# normal_pth = \"./LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern09-24--21-13-11/local_normal_save/\"\n",
    "attack_pth = \"/mnt/sda3/docker_space/Code/PHTDA-Net/LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern04-27--19-19-23/local_attack_save/\"\n",
    "normal_pth = \"/mnt/sda3/docker_space/Code/PHTDA-Net/LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern04-27--19-19-23/local_normal_save/\"\n",
    "global_pth = \"/mnt/sda3/docker_space/Code/PHTDA-Net/LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern04-27--19-19-23/fed/\"\n",
    "\n",
    "round_set = []\n",
    "client_set_attack = []\n",
    "client_set_normal = []\n",
    "modelpth_set = []\n",
    "gmodelpth_set = []\n",
    "for dirpath, dirnames, filenames in os.walk(attack_pth):\n",
    "    for filename in filenames:\n",
    "        # if int(re.findall(r'-?\\d+', filename)[0]) < round_max and int(re.findall(r'-?\\d+', filename)[1]) < client_max:\n",
    "        modelpth_set.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\"))\n",
    "        round_set.append(int(re.findall(r'-?\\d+', filename)[0]))\n",
    "        client_set_attack.append(int(re.findall(r'-?\\d+', filename)[1]))\n",
    "\n",
    "round_set = list(set(round_set))\n",
    "round_set.sort(reverse=True)\n",
    "round_set = [x for x in round_set if x >= 30]\n",
    "client_set_attack = list(set(client_set_attack))\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(normal_pth):\n",
    "    for filename in filenames:\n",
    "        # if int(re.findall(r'-?\\d+', filename)[0]) < round_max and int(re.findall(r'-?\\d+', filename)[1]) < client_max:\n",
    "        modelpth_set.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\"))\n",
    "        client_set_normal.append(int(re.findall(r'-?\\d+', filename)[1]))\n",
    "\n",
    "client_set_normal = list(set(client_set_normal))\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(global_pth):\n",
    "    for filename in filenames:\n",
    "        # if int(re.findall(r'-?\\d+', filename)[0]) < round_max and int(re.findall(r'-?\\d+', filename)[1]) < client_max:\n",
    "        gmodelpth_set.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\")) if \"model\" in filename else None\n",
    "\n",
    "print(\"rounds\", round_set, len(round_set))\n",
    "print(\"attack\", client_set_attack, len(client_set_attack))\n",
    "print(\"normal\", client_set_normal, len(client_set_normal))\n",
    "print(len(modelpth_set))\n",
    "print(len(gmodelpth_set))\n",
    "\n",
    "\n",
    "modelpth_dict = {}\n",
    "for idx, r in enumerate(round_set):\n",
    "    current_set_normal = []\n",
    "    current_set_attack = []\n",
    "    for mt in modelpth_set:\n",
    "        # print(mt)\n",
    "        if int(re.findall(r'-?\\d+', mt)[-2]) == r and int(re.findall(r'-?\\d+', mt)[-1]) in client_set_normal:\n",
    "            # print(int(re.findall(r'-?\\d+', mt)[-1]))\n",
    "            current_set_normal.append(mt)\n",
    "        if int(re.findall(r'-?\\d+', mt)[-2]) == r and int(re.findall(r'-?\\d+', mt)[-1]) in client_set_attack:\n",
    "            # print(int(re.findall(r'-?\\d+', mt)[-1]))\n",
    "            current_set_attack.append(mt)\n",
    "        \n",
    "    modelpth_dict[f'{r}_normal'] = current_set_normal\n",
    "    modelpth_dict[f'{r}_attack'] = current_set_attack\n",
    "# print(modelpth_dict)\n",
    "client_max = min(30,len(client_set_attack),len(client_set_normal))\n",
    "\n",
    "\n",
    "modelname = \"\"\n",
    "weight_keys_resnet = [\"conv1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.conv2.weight\", \"layer1.1.conv1.weight\", \"layer1.1.conv2.weight\", \"layer1.2.conv1.weight\", \"layer1.2.conv2.weight\", \"layer2.0.conv1.weight\", \"layer2.0.conv2.weight\",\n",
    " \"layer2.1.conv1.weight\", \"layer2.1.conv2.weight\", \"layer2.2.conv1.weight\", \"layer2.2.conv2.weight\", \"layer3.0.conv1.weight\", \"layer3.0.conv2.weight\", \"layer3.1.conv1.weight\", \"layer3.1.conv2.weight\", \"layer3.2.conv1.weight\", \"layer3.2.conv2.weight\"]\n",
    "\n",
    "if \"lenet\" in attack_pth:\n",
    "    modelname = \"lenet\"\n",
    "    weightsize = 50\n",
    "elif \"VGG\" in attack_pth:\n",
    "    modelname = \"vgg\"\n",
    "    weightsize = 64\n",
    "elif \"resnet\" in attack_pth:\n",
    "    modelname = \"resnet\"\n",
    "    weightsize = 16\n",
    "    weight_keys = weight_keys_resnet\n",
    "else:\n",
    "    assert 0==1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "344.03779315948486\n",
      "Files already downloaded and verified\n",
      "340.28492975234985\n",
      "Files already downloaded and verified\n",
      "326.54140853881836\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339.70978593826294\n",
      "4\n",
      "(60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "formatted_date_time = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "corr_set = []\n",
    "normal_nums = int(client_max)\n",
    "attack_nums = int(client_max)\n",
    "total_nums = normal_nums+attack_nums\n",
    "\n",
    "for idround, round_number in enumerate(round_set):\n",
    "    # if idround >= 1:\n",
    "    #     break\n",
    "    r_modelpth_set = modelpth_dict[f'{round_number}_normal'][0:normal_nums] + modelpth_dict[f'{round_number}_attack'][0:attack_nums]\n",
    "    r_corr = np.zeros((total_nums, total_nums, 1))\n",
    "\n",
    "    topofeaturelist = []\n",
    "    gradientlist = []\n",
    "    model, Dataloader, dataset = initDataset(modelname)\n",
    "    stime = time.time()\n",
    "    for idx, modelp1 in enumerate(r_modelpth_set):\n",
    "            gradientlist.append(getGradients(modelpth=modelp1, model=model, dataloader=Dataloader))\n",
    "            topofeaturelist.append(getTopofeature(modelpth=modelp1, model=model, dataloader=Dataloader, dataset=dataset))\n",
    "    print(f\"{time.time() - stime}\")\n",
    "\n",
    "    vectors = zip(gradientlist, topofeaturelist)\n",
    "    # for items in vectors:\n",
    "    #     print(items)\n",
    "    \n",
    "    for id1, v1 in enumerate(copy.deepcopy(vectors)):\n",
    "        if id1 >= total_nums:\n",
    "            break\n",
    "        local_grad_1 = [v1[0][key] for key in weight_keys]\n",
    "        for id2, v2 in enumerate(copy.deepcopy(vectors)):\n",
    "            if id2 >= total_nums:\n",
    "                break\n",
    "            if id2 <= id1:\n",
    "                continue\n",
    "            \n",
    "            local_grad_2 = [v2[0][key] for key in weight_keys]\n",
    "            corrlist = []\n",
    "\n",
    "            for layer_id, _ in enumerate(local_grad_1):\n",
    "                nd_vector = np.prod(local_grad_1[layer_id].cpu().shape)\n",
    "\n",
    "                ly1 = local_grad_1[layer_id].cpu().reshape(minimizeProduct(nd_vector)[0])\n",
    "                ly2 = local_grad_2[layer_id].cpu().reshape(minimizeProduct(nd_vector)[0])\n",
    "\n",
    "                l = linear_CKA(ly1.T, ly2.T)\n",
    "                k = kernel_CKA(ly1.T, ly2.T)\n",
    "\n",
    "                corrlist.append((l+k)/2)\n",
    "\n",
    "            nd_fv = np.prod(v1[1].shape)\n",
    "\n",
    "            fv1 = v1[1].reshape((nd_fv,-1))\n",
    "            fv2 = v2[1].reshape((nd_fv,-1))\n",
    "            lfv = linear_CKA(fv1, fv2)\n",
    "            kfv = linear_CKA(fv1, fv2)\n",
    "           \n",
    "            r_corr[id1][id2] = np.mean(np.array(corrlist))/2 + (lfv+kfv)/4\n",
    "    \n",
    "    corr_set.append([round_number, r_corr])\n",
    "\n",
    "\n",
    "\n",
    "print(len(corr_set))\n",
    "print(corr_set[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/corrset_temp_v3_5_2024-04-27-20-16-06_[36, 34, 32, 30]_[30_30].txt\n"
     ]
    }
   ],
   "source": [
    "# np.savetxt(f\"./data/corrset_temp_v3_5_{formatted_date_time}.txt\",np.array([row[1] for row in corr_set]).reshape(len(corr_set), -1))\n",
    "# print(formatted_date_time)\n",
    "np.savetxt(f\"./data/corrset_temp_v3_5_{formatted_date_time}_{round_set}_[{normal_nums}_{attack_nums}].txt\",np.array([row[1] for row in corr_set]).reshape(len(corr_set), -1))\n",
    "print(f\"./data/corrset_temp_v3_5_{formatted_date_time}_{round_set}_[{normal_nums}_{attack_nums}].txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "load_date_time = \"2023-12-24 23:32:47\"\n",
    "file_path = f\"./data/corrset_temp_{load_date_time}.txt\"\n",
    "\n",
    "\n",
    "corr_set_recovered = []\n",
    "normal_nums = int(client_max)\n",
    "attack_nums = int(client_max/5)\n",
    "total_nums = normal_nums+attack_nums\n",
    "\n",
    "total_rounds = len(round_set)\n",
    "loaded_data = np.loadtxt(file_path)\n",
    "corr_matrices = loaded_data.reshape(total_rounds, total_nums, total_nums, 1)\n",
    "\n",
    "for idx, matrix in enumerate(corr_matrices):\n",
    "    round_number = idx  # or fetch from a saved round number list if available\n",
    "    corr_set_recovered.append([round_number, matrix])\n",
    "\n",
    "if \"formatted_date_time\" not in globals() and \"formatted_date_time\" not in locals():\n",
    "    corr_set = corr_set_recovered\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pcode import isProperSuperset, findDifferentElements, flatten, findLargestGaps, grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for round_number, corr in corr_set:\n",
    "    data = 1 - corr.reshape(-1, total_nums) - corr.reshape(-1, total_nums).T\n",
    "    print(round_number, data.shape)\n",
    "    # df=pd.DataFrame(data)\n",
    "    # plot=seaborn.heatmap(df)\n",
    "    # plt.show()\n",
    "    sns.heatmap(1-data)\n",
    "    plt.show()\n",
    "\n",
    "    # Apply Multidimensional Scaling\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "    mds_results = mds.fit_transform(data)\n",
    "\n",
    "    # Plot the points\n",
    "    plt.scatter(mds_results[:, 0], mds_results[:, 1])\n",
    "\n",
    "    for i, points in enumerate(mds_results):\n",
    "        plt.annotate(i, (points[0], points[1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    plt.show()\n",
    "\n",
    "    dv_seq = grouping(data, total_nums, normal_nums, mds_results)\n",
    "\n",
    "    \n",
    "    X = np.array([x for x in dv_seq if x != float('Inf')])\n",
    "    X = X.reshape(-1, 1)\n",
    "\n",
    "    # 初始化KDE模型\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.01).fit(X)\n",
    "\n",
    "    # 进行密度估计\n",
    "    X_sample = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    log_dens = kde.score_samples(X_sample)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(X_sample, np.exp(log_dens), label='KDE (Gaussian)')\n",
    "    plt.title('Kernel Density Estimation')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # plt.scatter(mds_results[:, 0], mds_results[:, 1])\n",
    "    # for i, points in enumerate(mds_results):\n",
    "    #     plt.annotate(i, (points[0], points[1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
