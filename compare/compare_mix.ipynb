{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from utils.model_analysis_nets import LeNet, VGG16, ResNet20\n",
    "from utils.load import getGradients, getTotalLength, mergeSublistsWithSharedItems, initDataset, minimizeProduct\n",
    "from utils.load import getSamples, getTopofeature, extractWeights\n",
    "from CKA import linear_CKA, kernel_CKA\n",
    "import perscode\n",
    "\n",
    "import numpy as np\n",
    "import sympy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, fclusterdata\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import gif\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import gudhi as gd\n",
    "from ripser import Rips\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "trans_mnist = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trans_cifar10_val = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                             std=[0.229, 0.224, 0.225])])\n",
    "dataset_test = datasets.CIFAR10('../data/cifar10/', train=False, download=True, transform=trans_cifar10_val)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset_test, batch_size=32,\n",
    "                num_workers=2, pin_memory=True, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "colors = ['r', 'g', 'b', 'y', 'p']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounds [36, 34, 32, 30, 28, 26, 24, 22] 8\n",
      "attack [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39] 40\n",
      "normal [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 60\n",
      "640\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# attack_pth = \"./LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern09-24--21-13-11/local_attack_save/\"\n",
    "# normal_pth = \"./LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern09-24--21-13-11/local_normal_save/\"\n",
    "attack_pth = \"/mnt/sda3/docker_space/Code/PHTDA-Net/LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern12-16--19-06-13/local_attack_save/\"\n",
    "normal_pth = \"/mnt/sda3/docker_space/Code/PHTDA-Net/LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern12-16--19-06-13/local_normal_save/\"\n",
    "global_pth = \"/mnt/sda3/docker_space/Code/PHTDA-Net/LG-FedAvg/save_attack_ub/cifar10/resnet20_iidTrue_num100_C0.8_le2_DBATrue/shard2/pattern12-16--19-06-13/fed/\"\n",
    "\n",
    "round_set = []\n",
    "client_set_attack = []\n",
    "client_set_normal = []\n",
    "modelpth_set = []\n",
    "gmodelpth_set = []\n",
    "for dirpath, dirnames, filenames in os.walk(attack_pth):\n",
    "    for filename in filenames:\n",
    "        # if int(re.findall(r'-?\\d+', filename)[0]) < round_max and int(re.findall(r'-?\\d+', filename)[1]) < client_max:\n",
    "        modelpth_set.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\"))\n",
    "        round_set.append(int(re.findall(r'-?\\d+', filename)[0]))\n",
    "        client_set_attack.append(int(re.findall(r'-?\\d+', filename)[1]))\n",
    "\n",
    "round_set = list(set(round_set))\n",
    "round_set.sort(reverse=True)\n",
    "client_set_attack = list(set(client_set_attack))\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(normal_pth):\n",
    "    for filename in filenames:\n",
    "        # if int(re.findall(r'-?\\d+', filename)[0]) < round_max and int(re.findall(r'-?\\d+', filename)[1]) < client_max:\n",
    "        modelpth_set.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\"))\n",
    "        client_set_normal.append(int(re.findall(r'-?\\d+', filename)[1]))\n",
    "\n",
    "client_set_normal = list(set(client_set_normal))\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(global_pth):\n",
    "    for filename in filenames:\n",
    "        # if int(re.findall(r'-?\\d+', filename)[0]) < round_max and int(re.findall(r'-?\\d+', filename)[1]) < client_max:\n",
    "        gmodelpth_set.append(os.path.join(dirpath, filename).replace(\"\\\\\",\"/\")) if \"model\" in filename else None\n",
    "\n",
    "print(\"rounds\", round_set, len(round_set))\n",
    "print(\"attack\", client_set_attack, len(client_set_attack))\n",
    "print(\"normal\", client_set_normal, len(client_set_normal))\n",
    "print(len(modelpth_set))\n",
    "print(len(gmodelpth_set))\n",
    "\n",
    "\n",
    "modelpth_dict = {}\n",
    "for idx, r in enumerate(round_set):\n",
    "    current_set_normal = []\n",
    "    current_set_attack = []\n",
    "    for mt in modelpth_set:\n",
    "        # print(mt)\n",
    "        if int(re.findall(r'-?\\d+', mt)[-2]) == r and int(re.findall(r'-?\\d+', mt)[-1]) in client_set_normal:\n",
    "            # print(int(re.findall(r'-?\\d+', mt)[-1]))\n",
    "            current_set_normal.append(mt)\n",
    "        if int(re.findall(r'-?\\d+', mt)[-2]) == r and int(re.findall(r'-?\\d+', mt)[-1]) in client_set_attack:\n",
    "            # print(int(re.findall(r'-?\\d+', mt)[-1]))\n",
    "            current_set_attack.append(mt)\n",
    "        \n",
    "    modelpth_dict[f'{r}_normal'] = current_set_normal\n",
    "    modelpth_dict[f'{r}_attack'] = current_set_attack\n",
    "# print(modelpth_dict)\n",
    "client_max = min(30,len(client_set_attack),len(client_set_normal))\n",
    "\n",
    "\n",
    "modelname = \"\"\n",
    "weight_keys_resnet = [\"conv1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.conv2.weight\", \"layer1.1.conv1.weight\", \"layer1.1.conv2.weight\", \"layer1.2.conv1.weight\", \"layer1.2.conv2.weight\", \"layer2.0.conv1.weight\", \"layer2.0.conv2.weight\",\n",
    " \"layer2.1.conv1.weight\", \"layer2.1.conv2.weight\", \"layer2.2.conv1.weight\", \"layer2.2.conv2.weight\", \"layer3.0.conv1.weight\", \"layer3.0.conv2.weight\", \"layer3.1.conv1.weight\", \"layer3.1.conv2.weight\", \"layer3.2.conv1.weight\", \"layer3.2.conv2.weight\"]\n",
    "\n",
    "if \"lenet\" in attack_pth:\n",
    "    modelname = \"lenet\"\n",
    "    weightsize = 50\n",
    "elif \"VGG\" in attack_pth:\n",
    "    modelname = \"vgg\"\n",
    "    weightsize = 64\n",
    "elif \"resnet\" in attack_pth:\n",
    "    modelname = \"resnet\"\n",
    "    weightsize = 16\n",
    "    weight_keys = weight_keys_resnet\n",
    "else:\n",
    "    assert 0==1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "file_path = \"/mnt/sda3/docker_space/Code/PHTDA-Net/data/corrset_temp_v4_5_2024-01-10 21:44:40.txt\"\n",
    "\n",
    "\n",
    "corr_set_recovered = []\n",
    "normal_nums = int(client_max)\n",
    "attack_nums = int(client_max/5)\n",
    "total_nums = normal_nums+attack_nums\n",
    "\n",
    "total_rounds = len(round_set)\n",
    "loaded_data = np.loadtxt(file_path)\n",
    "corr_matrices = loaded_data.reshape(total_rounds, total_nums, total_nums, 1)\n",
    "\n",
    "for idx, matrix in enumerate(corr_matrices):\n",
    "    round_number = idx  # or fetch from a saved round number list if available\n",
    "    corr_set_recovered.append([round_number, matrix])\n",
    "\n",
    "corr_set = corr_set_recovered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_distance(data, k):\n",
    "    \"\"\"\n",
    "    Compute the k-distance graph for a given distance matrix.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): An n x n distance matrix.\n",
    "    k (int): The number of neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Sorted k-distances for all points.\n",
    "    \"\"\"\n",
    "    # Ensure the diagonal is infinity, as we don't consider a point's distance to itself\n",
    "    np.fill_diagonal(data, np.inf)\n",
    "\n",
    "    # Sort each row and take the k-th nearest distance\n",
    "    sorted_distances = np.sort(data, axis=1)\n",
    "    k_distances = sorted_distances[:, k-1]\n",
    "\n",
    "    # Sort the k-distances of all points\n",
    "    sorted_k_distances = np.sort(k_distances)\n",
    "\n",
    "    return sorted_k_distances\n",
    "\n",
    "# Example usage:\n",
    "# Let's assume `distance_matrix` is your nxn distance matrix and you choose k = 4\n",
    "# distance_matrix = np.random.rand(100, 100)  # Replace with your actual distance matrix\n",
    "# k = 4\n",
    "# sorted_k_distances = k_distance(distance_matrix, k)\n",
    "\n",
    "# Plotting the k-distance graph\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(sorted_k_distances)\n",
    "# plt.xlabel('Points')\n",
    "# plt.ylabel(f'{k}-Distance')\n",
    "# plt.title(f'{k}-Distance Graph')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# Note: Uncomment and modify the above code according to your `distance_matrix` and desired `k` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (36, 36)\n",
      "Cluster assignments: [1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1]\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'eps' parameter of DBSCAN must be a float in the range (0.0, inf). Got array([0.07683231, 0.08134704, 0.08214327, 0.08333694, 0.08333694,\n       0.08411538, 0.08521945, 0.08910885, 0.09049283, 0.09355645,\n       0.09394946, 0.10182115, 0.1020674 , 0.10337318, 0.10374436,\n       0.11168118, 0.11207994, 0.11387125, 0.11417669, 0.11468194,\n       0.1191246 , 0.11961225, 0.12120875, 0.12590619, 0.12819446,\n       0.13205397, 0.13432142, 0.14032312, 0.14434683, 0.14514125,\n       0.15617006, 0.16191721, 0.16628059, 0.1671337 , 0.22235378,\n       0.23082795]) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCluster assignments:\u001b[39m\u001b[39m\"\u001b[39m, labels_kmeans)\n\u001b[1;32m     19\u001b[0m dbscan \u001b[39m=\u001b[39m DBSCAN(eps\u001b[39m=\u001b[39mk_distance(data,\u001b[39m5\u001b[39m), min_samples\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m labels_dbscan \u001b[39m=\u001b[39m dbscan\u001b[39m.\u001b[39;49mfit_predict(data)\n\u001b[1;32m     22\u001b[0m gmm \u001b[39m=\u001b[39m GaussianMixture(n_components\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/sklearn/cluster/_dbscan.py:448\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_predict\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/sklearn/base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m partial_fit_and_fitted \u001b[39m=\u001b[39m (\n\u001b[1;32m   1140\u001b[0m     fit_method\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpartial_fit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1141\u001b[0m )\n\u001b[1;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m global_skip_validation \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1144\u001b[0m     estimator\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[1;32m   1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/sklearn/base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    630\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \n\u001b[1;32m    632\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[1;32m    639\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    640\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[1;32m    641\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'eps' parameter of DBSCAN must be a float in the range (0.0, inf). Got array([0.07683231, 0.08134704, 0.08214327, 0.08333694, 0.08333694,\n       0.08411538, 0.08521945, 0.08910885, 0.09049283, 0.09355645,\n       0.09394946, 0.10182115, 0.1020674 , 0.10337318, 0.10374436,\n       0.11168118, 0.11207994, 0.11387125, 0.11417669, 0.11468194,\n       0.1191246 , 0.11961225, 0.12120875, 0.12590619, 0.12819446,\n       0.13205397, 0.13432142, 0.14032312, 0.14434683, 0.14514125,\n       0.15617006, 0.16191721, 0.16628059, 0.1671337 , 0.22235378,\n       0.23082795]) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "for round_number, corr in corr_set:\n",
    "    data = 1 - corr.reshape(-1, total_nums) - corr.reshape(-1, total_nums).T\n",
    "    print(round_number, data.shape)\n",
    "    # df=pd.DataFrame(data)\n",
    "    # plot=seaborn.heatmap(df)\n",
    "    # plt.show()\n",
    "    # sns.heatmap(1-data)\n",
    "    # plt.show()\n",
    "\n",
    "    k = 2\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(data)\n",
    "    labels_kmeans = kmeans.labels_\n",
    "    \n",
    "    print(\"Cluster assignments:\", labels_kmeans)\n",
    "\n",
    "    dbscan = DBSCAN(eps=k_distance(data,5), min_samples=5, metric='precomputed')\n",
    "    labels_dbscan = dbscan.fit_predict(data)\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=10, random_state=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
